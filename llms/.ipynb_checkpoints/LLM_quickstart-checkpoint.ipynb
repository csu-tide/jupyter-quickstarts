{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a68d5c-4347-4409-9fac-312325bc3443",
   "metadata": {},
   "source": [
    "# Large Language Model in JupyterHub\n",
    "#### *Difficulty: Easy*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddd9de-c2ec-4b68-898c-08e81206bf88",
   "metadata": {},
   "source": [
    "### ðŸŒŽ Overview\n",
    "In this quickstart, we'll run [**Ollama**](https://github.com/ollama/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6eed6-a592-42b7-bc3d-812c73fe3fea",
   "metadata": {},
   "source": [
    "### ðŸ§  Prerequisites\n",
    "- Jupyter Notebooks\n",
    "- Python\n",
    "- Basic CLI commands\n",
    "- Ollama llama3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f566c-7e40-4612-880b-94d5dc355c8c",
   "metadata": {},
   "source": [
    "### âœ” Learning Outcomes\n",
    "1. Understand the basics of Large Language Models (LLMs)\n",
    "2. Learn how to interact with the Ollama Llama3 model\n",
    "3. Explore examples of using LLMs for tasks like code generation, translation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b61e9-903d-4eae-9f32-8515dc315e32",
   "metadata": {},
   "source": [
    "### ðŸ’¾ Resource Requirements\n",
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <th>Option</th>\n",
    "        <th>Selection</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>GPU type</td>\n",
    "        <td>L40</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>GPUs</td>\n",
    "        <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>CPU</td>\n",
    "        <td>4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>RAM</td>\n",
    "        <td>16</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Image</td>\n",
    "        <td>LLM Notebook</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834989db-c7a2-4ba6-8bfa-52a226bfa445",
   "metadata": {},
   "source": [
    "### ðŸ“š Libraries\n",
    "Inorder to start with Ollama llama3 model basic requirement is installation of ollama using commad line \n",
    "\n",
    "## 1. Starting Ollama\n",
    "Repeat these steps each time you start your notebook:\n",
    "\n",
    "1. Click the \"+\" icon in the top left.(i.e Launcher)\n",
    "2. From the start page, select the Terminal icon to pull up a Linux terminal.\n",
    "3. Run the command `ollama serve`.\n",
    "4. Leave this terminal window running as you continue your work.\n",
    "\n",
    "## 2. Downloading Models\n",
    "Repeat these steps each time you need to download models:\n",
    "1. Click the \"+\" icon in the top left.\n",
    "2. From the start page, select the Terminal icon to pull up a Linux terminal.\n",
    "3. Check the [Ollama site](https://ollama.com) for available models.\n",
    "4. Pull down models using the command `ollama pull [model-name-here]`.\n",
    "   - For example: `ollama pull llama3`.\n",
    "5. Repeat steps 3-4 for each model you would like to download.\n",
    "6. Verify all downloaded models with the command `ollama list`.\n",
    "\n",
    "Ensure all desired models are successfully downloaded and located under `/home/jovyan/.ollama/models`.\n",
    "\n",
    "# Running LLMs with Ollama\n",
    "\n",
    "## 3. OpenAI API Example\n",
    "To run the OpenAI API example:\n",
    "\n",
    "1. Click the \"+\" icon in the top left.\n",
    "2. From the start page, select the Terminal icon to pull up a Linux terminal.\n",
    "3. Clone the repository into your notebook:\n",
    "   git clone *link to be added*\n",
    "4. Navigate to the repository directory via the file explorer.\n",
    "5. Open the example notebook `LLM_quickstart` and ensure you are running it with the \"Python [conda env:llm]\" kernel.\n",
    "6. Modify the model and message content as appropriate for one of the downloaded models.\n",
    "7. Execute the cell to verify functionality. Note: this may take a few moments if the LLM is not already loaded onto the GPU.\n",
    "\n",
    "## Commandline Interaction\n",
    "For commandline interaction with Ollama:\n",
    "\n",
    "1. Click the \"+\" icon in the top left.\n",
    "2. From the start page, select the Terminal icon to pull up a Linux terminal.\n",
    "3. Run the command:`ollama run [model-name-here]`\n",
    "   For example: `ollama run llama3`\n",
    "4. Wait for the model to load onto the GPU.\n",
    "5. Enter messages and hit enter to send them to the LLM.\n",
    "6. To exit commandline interaction, enter `/bye`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b661eb1-b3e9-4d08-8749-7dbcff95e648",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This Python script demonstrates how to use OpenAI's API via the Ollama platform to generate chat completions using the Llama3 model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d3f1d2e-bd94-451d-818d-7f8ad7a11256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WOOHOO!\\n\\nHello there, fellow learner!\\n\\nI\\'m super excited to have you on board for this LLM (Large Language Model) Quickstart using the amazing OLLAMA LLaMA-3 model!\\n\\nBefore we dive in, I\\'d like to clarify that:\\n\\n1. This quickstart is designed for beginners, so don\\'t worry if you\\'re new to AI or NLP.\\n2. We\\'ll be working with the OLLAMA LLaMA-3 model, which is a pre-trained language model that can be used for various tasks like text generation, language translation, and more.\\n\\nNow, let\\'s get started!\\n\\n**What are we going to cover?**\\n\\n1. Basic understanding of LLMs and their applications.\\n2. Introduction to the OLLAMA LLaMA-3 model and its capabilities.\\n3. Hands-on experience with LLaMA-3 using Python and the Hugging Face Transformers library.\\n\\n**What do you need to get started?**\\n\\n1. A basic understanding of Python programming (don\\'t worry if you\\'re rusty; I\\'ll guide you through it).\\n2. A computer with a stable internet connection.\\n3. The Hugging Face Transformers library installed (I can help you with this step).\\n\\nAre you ready to start our LLM Quickstart adventure?\\n\\nPlease respond with \"YES\" or \"READY\" to confirm, and we\\'ll begin our journey!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "\n",
    "    # required but ignored\n",
    "    api_key='ollama',\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'Say hello to learners and let\\'s get started with LLM quickstart using Ollama Llama3 model.',\n",
    "        }\n",
    "    ],\n",
    "    model='llama3',\n",
    ")\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229e2cf-6ebe-4f37-aa99-33891e8a4276",
   "metadata": {},
   "source": [
    "## Using Ollama to Chat with Llama3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07dffb5e-e7e2-47f3-8692-296760566161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Large Language Model (LLM) is a type of artificial intelligence (AI) model that is designed to process and generate large amounts of natural language text data. LLMs are trained on massive datasets of text, which enables them to learn complex patterns and relationships in language.\n",
      "\n",
      "LLMs are characterized by their ability to:\n",
      "\n",
      "1. **Understand**: They can comprehend the meaning of text at a sentence or paragraph level, including nuances such as context, tone, and sentiment.\n",
      "2. **Generate**: They can create new text that is coherent, fluent, and often indistinguishable from human-written text.\n",
      "3. **Reason**: They can draw logical conclusions based on the input they receive, making them useful for tasks like question answering, dialogue generation, and text classification.\n",
      "\n",
      "Some key features of LLMs include:\n",
      "\n",
      "1. **Scale**: They are trained on massive datasets, often in the order of tens or hundreds of millions of parameters.\n",
      "2. **Complexity**: They can learn complex relationships between words, phrases, and sentences.\n",
      "3. **Flexibility**: They can be fine-tuned for specific tasks or domains, such as conversational AI, language translation, or content generation.\n",
      "\n",
      "Examples of LLMs include:\n",
      "\n",
      "1. BERT (Bidirectional Encoder Representations from Transformers)\n",
      "2. RoBERTa (Robustly Optimized BERT Pretraining Approach)\n",
      "3. XLNet\n",
      "4. Transformer-XL\n",
      "\n",
      "LLMs have numerous applications, including:\n",
      "\n",
      "1. **Conversational AI**: They can power chatbots, virtual assistants, and other conversational interfaces.\n",
      "2. **Natural Language Processing (NLP)**: They are used in NLP tasks such as text classification, sentiment analysis, and question answering.\n",
      "3. **Content Generation**: They can generate text for various purposes, including marketing, entertainment, and education.\n",
      "4. **Language Translation**: They can be used to translate text from one language to another.\n",
      "\n",
      "The development of LLMs has led to significant advancements in AI research and has many potential applications across industries.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.chat(model='llama3', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'What is Large Language Model ?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b562f58d-17e4-434d-a365-b1b18826b125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa06f06a04c94bb59e23b2af58fcb11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Query:', placeholder='Type your query here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920e1756c6bc4582bdbd1845b17375f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Get Response', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e9c28614b34e399eb75ab7cd96c7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import ollama\n",
    "\n",
    "# Function to get response from the model\n",
    "def get_response(prompt):\n",
    "    # Define the chat messages\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': prompt,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Query the model\n",
    "    response = ollama.chat(model='llama3', messages=messages)\n",
    "\n",
    "    # Extract and return the response content\n",
    "    if 'message' in response:\n",
    "        return response['message']['content']\n",
    "    else:\n",
    "        return \"No response received.\"\n",
    "\n",
    "# Interactive function to get user input and display response\n",
    "def interact_with_model():\n",
    "    from IPython.display import display, clear_output\n",
    "    import ipywidgets as widgets\n",
    "\n",
    "    # Create input box for user query\n",
    "    query_box = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Type your query here',\n",
    "        description='Query:',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    # Create output area for displaying the response\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    # Function to handle button click\n",
    "    def on_button_click(b):\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            user_query = query_box.value\n",
    "            response = get_response(user_query)\n",
    "            print(\"Response from Llama3 model:\")\n",
    "            print(response)\n",
    "\n",
    "    # Create a button\n",
    "    button = widgets.Button(description=\"Get Response\")\n",
    "    button.on_click(on_button_click)\n",
    "\n",
    "    # Display the input box, button, and output area\n",
    "    display(query_box, button, output_area)\n",
    "\n",
    "# Call the interactive function to display widgets\n",
    "interact_with_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a07ca6c0-75b8-4132-a8da-846b4d1b4ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15bbc74e69d490290eef6b393fdddb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Category:', options=('Science', 'Technology', 'History', 'Literature', 'Health'), value=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73af50c9e844e639a3bf394e2d36629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Question:', placeholder='Type your question here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2904854166a46e28dd2fb6a6c741539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Get Answer', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb243bb2bcd40beb008239dde29ead7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import ollama\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Function to get response from the model\n",
    "def get_response(category, query):\n",
    "    # Define the chat messages\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f\"Category: {category}\\nQuestion: {query}\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Query the model\n",
    "    response = ollama.chat(model='llama3', messages=messages)\n",
    "\n",
    "    # Extract and return the response content\n",
    "    if 'message' in response:\n",
    "        return response['message']['content']\n",
    "    else:\n",
    "        return \"No response received.\"\n",
    "\n",
    "# Interactive function to handle user input and display response\n",
    "def interact_with_model():\n",
    "    # Define categories\n",
    "    categories = ['Science', 'Technology', 'History', 'Literature', 'Health']\n",
    "\n",
    "    # Create dropdown for category selection\n",
    "    category_dropdown = widgets.Dropdown(\n",
    "        options=categories,\n",
    "        value=categories[0],\n",
    "        description='Category:',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    # Create input box for user query\n",
    "    query_box = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Type your question here',\n",
    "        description='Question:',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    # Create output area for displaying the response\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    # Function to handle button click\n",
    "    def on_button_click(b):\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            category = category_dropdown.value\n",
    "            user_query = query_box.value\n",
    "            response = get_response(category, user_query)\n",
    "            print(f\"Category: {category}\\nQuestion: {user_query}\")\n",
    "            print(\"\\nResponse from Llama3 model:\")\n",
    "            print(response)\n",
    "\n",
    "    # Create a button\n",
    "    button = widgets.Button(description=\"Get Answer\")\n",
    "    button.on_click(on_button_click)\n",
    "\n",
    "    # Display the dropdown, input box, button, and output area\n",
    "    display(category_dropdown, query_box, button, output_area)\n",
    "\n",
    "# Call the interactive function to display widgets\n",
    "interact_with_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12f81099-3d26-44ef-8b37-492641c60730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing a binary search algorithm in Python is straightforward. Here's the basic idea:\n",
      "\n",
      "1.  Start by assuming that the target value is within the given range.\n",
      "2.  Calculate the middle index of the array and check if it's equal to your target value. If it is, return its index.\n",
      "3.  If not, consider two cases:\n",
      "    *   The target value is less than the middle element, so we know that the target value must be in the left half of the array.\n",
      "    *   The target value is greater than the middle element, so we know that the target value must be in the right half of the array.\n",
      "\n",
      "4.  Repeat steps 2 and 3 for the relevant half until you find your target value or it becomes clear that the target value doesn't exist within the array.\n",
      "\n",
      "Here's the Python code:\n",
      "\n",
      "```\n",
      "def binary_search(arr, target):\n",
      "    low = 0\n",
      "    high = len(arr) - 1\n",
      "\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        guess = arr[mid]\n",
      "        \n",
      "        if guess == target:\n",
      "            return mid\n",
      "        elif guess > target:\n",
      "            high = mid - 1\n",
      "        else:\n",
      "            low = mid + 1\n",
      "    return None\n",
      "\n",
      "\n",
      "# Example usage\n",
      "arr = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "target = 5\n",
      "\n",
      "result = binary_search(arr, target)\n",
      "\n",
      "if result is not None:\n",
      "    print(f\"Target {target} found at index {result}.\")\n",
      "else:\n",
      "    print(\"Target not found in the array.\")\n",
      "```\n",
      "\n",
      "In this code:\n",
      "\n",
      "-   The `binary_search` function takes an array and a target value as input.\n",
      "-   It maintains two pointers, `low` and `high`, that represent the current search range within the array. Initially, these pointers are set to the start (0) and end (`len(arr) - 1`) of the array, respectively.\n",
      "-   The function repeatedly calculates the midpoint (`mid`) of the search range using integer division (`//`).\n",
      "-   It then checks if the value at the midpoint (`guess`) is equal to the target. If it is, the function returns the index of the target.\n",
      "-   If not, it compares `guess` with the target:\n",
      "    *   If `guess > target`, the target must be in the left half (from `low` to `mid - 1`), so it updates `high` to `mid - 1`.\n",
      "    *   If `guess < target`, the target must be in the right half (from `mid + 1` to `high`), so it updates `low` to `mid + 1`.\n",
      "-   The function continues this process until it finds the target or determines that it's not present in the array. In the latter case, it returns `None`.\n",
      "\n",
      "When you run this code with the example usage provided, it should output:\n",
      "\n",
      "```\n",
      "Target 5 found at index 4.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Define the chat messages\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'How do you implement a binary search algorithm in Python?',\n",
    "    }\n",
    "]\n",
    "\n",
    "# Query the model\n",
    "response = ollama.chat(model='llama3', messages=messages)\n",
    "\n",
    "# Print the response from the model\n",
    "if 'message' in response:\n",
    "    print(response['message']['content'])\n",
    "else:\n",
    "    print(\"No response received.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db2479bd-a974-4dd6-a498-989aae2e680a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e38aa9ac3f463e904153f72c1110bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Query:', layout=Layout(height='100px', width='50%'), placeholder='Enter a progâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c6d21b4c924b13b133a05c384a238a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Code Example', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4884d3e19dd04b73befd9ce42ddcee53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import ollama\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Function to generate code examples from the model\n",
    "def generate_code_example(query):\n",
    "    # Define the chat messages\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f\"Query: {query}\\nGenerate code example for '{query}':\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Query the model\n",
    "    response = ollama.chat(model='llama3', messages=messages)\n",
    "\n",
    "    # Extract and return the response content\n",
    "    if 'message' in response:\n",
    "        return response['message']['content']\n",
    "    else:\n",
    "        return \"No response received.\"\n",
    "\n",
    "# Interactive function to handle user input and display code examples\n",
    "def interact_with_model():\n",
    "    # Create a text input for the user's query\n",
    "    query_input = widgets.Textarea(\n",
    "        value='',\n",
    "        placeholder='Enter a programming topic or query',\n",
    "        description='Query:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='50%', height='100px')\n",
    "    )\n",
    "    \n",
    "    # Create a button to generate code examples\n",
    "    button = widgets.Button(description=\"Generate Code Example\")\n",
    "    \n",
    "    # Create an output area for displaying the code examples\n",
    "    output_area = widgets.Output(layout={'border': '1px solid black', 'width': '70%', 'height': '300px', 'overflow_y': 'scroll'})\n",
    "    \n",
    "    # Function to handle button click\n",
    "    def on_button_click(b):\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            query = query_input.value.strip()\n",
    "            if query:\n",
    "                code_example = generate_code_example(query)\n",
    "                display(HTML(f\"<pre>{code_example}</pre>\"))\n",
    "            else:\n",
    "                print(\"Please enter a programming topic or query.\")\n",
    "    \n",
    "    # Link button click event\n",
    "    button.on_click(on_button_click)\n",
    "    \n",
    "    # Display widgets\n",
    "    display(query_input, button, output_area)\n",
    "\n",
    "# Call the interactive function to display widgets\n",
    "interact_with_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f0cb09a-f154-4c18-9792-1054b5c1fe25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e29504fc1e4835ba5fc82e2056fa1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='From:', options=('English', 'Spanish', 'French', 'German', 'Chinese'), value='English')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea9a6a87ec5425d92fa3e04906d9340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='To:', index=1, options=('English', 'Spanish', 'French', 'German', 'Chinese'), value='Spaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b955ad6dd37e492f87c8ff82e150a263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Text:', layout=Layout(height='100px', width='50%'), placeholder='Type your texâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff144718cf5644d88972147dd19f5282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Translate', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8321ceb5246d4fb69d39d5e7ec3e4813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import ollama\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Function to get translation from the model\n",
    "def get_translation(source_lang, target_lang, text):\n",
    "    # Define the chat messages\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f\"Translate the following text from {source_lang} to {target_lang}: {text}\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Query the model\n",
    "    response = ollama.chat(model='llama3', messages=messages)\n",
    "\n",
    "    # Extract and return the response content\n",
    "    if 'message' in response:\n",
    "        return response['message']['content']\n",
    "    else:\n",
    "        return \"No response received.\"\n",
    "\n",
    "# Interactive function to handle user input and display translation\n",
    "def interact_with_model():\n",
    "    # Define language options\n",
    "    languages = ['English', 'Spanish', 'French', 'German', 'Chinese']\n",
    "\n",
    "    # Create dropdowns for language selection\n",
    "    source_lang_dropdown = widgets.Dropdown(\n",
    "        options=languages,\n",
    "        value=languages[0],\n",
    "        description='From:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    \n",
    "    target_lang_dropdown = widgets.Dropdown(\n",
    "        options=languages,\n",
    "        value=languages[1],\n",
    "        description='To:',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    # Create input box for user text\n",
    "    text_box = widgets.Textarea(\n",
    "        value='',\n",
    "        placeholder='Type your text here',\n",
    "        description='Text:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='50%', height='100px')\n",
    "    )\n",
    "\n",
    "    # Create output area for displaying the translation\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    # Function to handle button click\n",
    "    def on_button_click(b):\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            source_lang = source_lang_dropdown.value\n",
    "            target_lang = target_lang_dropdown.value\n",
    "            user_text = text_box.value\n",
    "            translation = get_translation(source_lang, target_lang, user_text)\n",
    "            print(f\"Translating from {source_lang} to {target_lang}:\\n\")\n",
    "            print(translation)\n",
    "\n",
    "    # Create a button\n",
    "    button = widgets.Button(description=\"Translate\")\n",
    "    button.on_click(on_button_click)\n",
    "\n",
    "    # Display the dropdowns, input box, button, and output area\n",
    "    display(source_lang_dropdown, target_lang_dropdown, text_box, button, output_area)\n",
    "\n",
    "# Call the interactive function to display widgets\n",
    "interact_with_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6094c3c-0942-4b44-81ca-862b7b8a6f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llm]",
   "language": "python",
   "name": "conda-env-llm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
