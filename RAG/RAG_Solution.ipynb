{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa97a95-52c5-4e58-81f7-ed62c916ce27",
   "metadata": {},
   "source": [
    "# RAG â€“ Retrieval Augmented Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ac02ce-b9e4-45e8-a355-9121b514fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain -q\n",
    "!pip install langchain_community -q\n",
    "!pip install chromadb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0922911-8123-41a2-85b9-f7b83e725776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d0f8c-b30a-4e71-9e0b-b1f431c5372b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this RAG tutorial, we'll be working with [LangChain](https://python.langchain.com/v0.2/docs/introduction/), which is a powerful framework for building applications with language models. LangChain provides utilities for working with various language model providers, integrating embeddings, and creating chains for more complex applications. Below are the necessary imports for this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ca64f-e1ee-4a9d-bf54-57e8c195d04a",
   "metadata": {},
   "source": [
    "We are also using [Ollama](https://ollama.com/), which is a platform for running LLMs on your local machine.  The following steps are needed to set up ollama for this RAG tutorial:\n",
    "1. In a terminal window in JupyterLab, type in the following command to start up the ollama service:\n",
    "**ollama serve**\n",
    "2. In another terminal window in JupyterLab, type in the following to download the model:  **ollama pull mistral**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c28c5e-392e-4ae0-8afe-dd663f32ce83",
   "metadata": {},
   "source": [
    "## Part 1: Retrieval\n",
    "\n",
    "- In this section, we'll focus on the retrieval aspect of RAG. We'll start by understanding vectorization, followed by storing and retrieving vectors efficiently.\n",
    "\n",
    "#### Vectorizing\n",
    "\n",
    "- **Vectorization** is the process of converting text into vectors in an embedding space. These vectors capture the semantic meaning of the text, enabling us to perform various operations like similarity calculations. We'll use HuggingFaceEmbeddings for this task, which is a class in LangChain that allows for an **embedding model** from HuggingFace to be integrated into LangChain applications. You can see the documentation for this LangChain class [here](https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.huggingface.HuggingFaceEmbeddings.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2230272b-ba40-41c7-b3d2-1356a6286084",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-keras --upgrade -q\n",
    "!pip install --upgrade transformers sentence-transformers langchain_community -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f763639e-a34c-4223-a11c-b838ae29f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize the vectorizer\n",
    "vectorizer = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258be401-ce04-43e0-9400-8a51bfb69a9f",
   "metadata": {},
   "source": [
    "This vectorizer converts text into vectors in embedding space. Lets try seeing how we can use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5d355ea-1406-4da7-9778-f11efd6062e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.05314696952700615,\n",
       " 0.014194391667842865,\n",
       " 0.007145767565816641,\n",
       " 0.06860870122909546,\n",
       " -0.0784803181886673,\n",
       " 0.010167491622269154,\n",
       " 0.10228311270475388,\n",
       " -0.012064853683114052,\n",
       " 0.09521345049142838,\n",
       " -0.030350126326084137]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.embed_query(\"dog\")[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ed0d1-412b-40dc-9a60-6ac60e88cf3d",
   "metadata": {},
   "source": [
    "- As you can see from above, this converts text into a series of numbers. \n",
    "\n",
    "### Task 1\n",
    "\n",
    "Your job is to **write a function that takes in two strings, vectorize them, and return their cosine similarity.** Implement the following function.\n",
    "\n",
    "#### `similarity_two_queries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61d9536c-aef6-4af4-9c7b-8611d4c92652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_two_queries(word1, word2):\n",
    "    # TODO\n",
    "    word1_vec = vectorizer.embed_query(word1)\n",
    "    word2_vec = vectorizer.embed_query(word2)\n",
    "    return np.dot(word1_vec,word2_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0521f6f-630e-4e40-b66e-957c8e00f110",
   "metadata": {},
   "source": [
    "- Observe the similarity scores of both **'cat'** and **'dog'** to the word **'kitten'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22c99caa-1b2f-4d53-92f3-140480908aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of 'kitten' and 'cat':  0.7882108523104192\n",
      "Similarity of 'kitten' and 'dog':  0.5205050432579855\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity of 'kitten' and 'cat': \",similarity_two_queries(\"kitten\",\"cat\"))\n",
    "print(\"Similarity of 'kitten' and 'dog': \",similarity_two_queries(\"kitten\",\"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f121272b-fe4a-488c-89ed-320a6ae5dd24",
   "metadata": {},
   "source": [
    "- By using the previously defined function,  we can take pairs of texts and quantify how **similar** they are.\n",
    "\n",
    "### Task 2\n",
    "\n",
    "**Which of the following words in the list `words` are most related to the word 'color'?** The function `similarity_list` takes a list of words, and outputs the word and similarity score from highest to lowest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ec8100d-03b8-4b5e-beee-1959fc5250bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_list(word,list):\n",
    "    similarity_list = [(i,similarity_two_queries(\"color\",i)) for i in words]\n",
    "    sorted_similarity_list = sorted(similarity_list,key=lambda x:x[1],reverse=True)\n",
    "    return sorted_similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d23161b1-5eb0-4646-a3e0-0d757873082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"rainbow\",\"car\",\"black\",\"red\",\"cat\",\"tree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b8e6f85-cb59-4739-a0f5-960e9c212315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('black', 0.7855719945035862),\n",
       " ('red', 0.7491879989606778),\n",
       " ('rainbow', 0.5601087130572295),\n",
       " ('car', 0.4040626729451856),\n",
       " ('cat', 0.35839052763321166),\n",
       " ('tree', 0.35735521995380115)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_list(\"color\",words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408298c-e28c-4876-bc11-4d90510b9bf2",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Each query below has an appropriate text that allows you to answer the question. The function `match_queries_with_texts` matches a query with its most related text. **Come up with 3 more questions and 3 suitable answers and add them to the list below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f6b1718-3887-4923-8710-41f3ab0d64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_queries_with_texts(queries, texts):\n",
    "    # Calculate similarities between each query and text\n",
    "    similarities = np.zeros((len(queries), len(texts)))\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        for j, text in enumerate(texts):\n",
    "            similarities[i, j] = similarity_two_queries(query, text)\n",
    "    \n",
    "    # Match each query to the text with the highest similarity\n",
    "    matches = {}\n",
    "    for i, query in enumerate(queries):\n",
    "        best_match_idx = np.argmax(similarities[i])\n",
    "        matches[query] = texts[best_match_idx]\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f575526f-49eb-4beb-8ee0-f61bf0ec04c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"What are the 7 colors of the rainbow?\", \n",
    "           \"What does Elsie do for work?\", \n",
    "           \"Which country has the largest population?\",\n",
    "           \"What time is it?\",\n",
    "           \"What is the largest continent?\",\n",
    "           \"Who is the greatest Football player?\"]\n",
    "texts = [\"China has 1.4 billion people.\",\n",
    "         \"Elsie works the register at Arby's.\", \n",
    "         \"The colors of the rainbow are ROYGBIV.\",\n",
    "         \"The time is 3:14.\",\n",
    "         \"The largest continent is Asia.\",\n",
    "         \"Christiano Ronaldo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7f7c6-944b-4142-b3ec-546e9fc63920",
   "metadata": {},
   "source": [
    "- Now we shuffle the queries and texts. Let's see if we can match them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7475dd2e-05e4-4ec8-9de4-006edfa606c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Who is the greatest Football player?': 'Christiano Ronaldo',\n",
       " 'What is the largest continent?': 'The largest continent is Asia.',\n",
       " 'What does Elsie do for work?': \"Elsie works the register at Arby's.\",\n",
       " 'What time is it?': 'The time is 3:14.',\n",
       " 'Which country has the largest population?': 'China has 1.4 billion people.',\n",
       " 'What are the 7 colors of the rainbow?': 'The colors of the rainbow are ROYGBIV.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(queries)\n",
    "random.shuffle(texts)\n",
    "\n",
    "match_queries_with_texts(queries, texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24525a35-9b13-46b3-9aa2-8dbe48714dfb",
   "metadata": {},
   "source": [
    "#### Database\n",
    "\n",
    "Now lets look at how we can store these for efficient retrieval of the vectors. There are many options for storage but in this exercise, we use [ChromaDB](https://python.langchain.com/v0.1/docs/integrations/vectorstores/chroma/)\n",
    " which is an open-source vector DB.\n",
    "\n",
    "Through langchain, we can set the database to be a LangChain ***retriever*** object, which essentially allows us to perform queries similarly to what we have done before.\n",
    "\n",
    "**Taking the `texts` and `queries` that you defined before, we can load it into ChromaDB and similarly perform the same operations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ca3bf18-a3c8-421a-9bf1-622372a250b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(range(len(texts)))\n",
    "db = Chroma.from_texts(texts, vectorizer, metadatas=[{\"id\": id} for id in ids])\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59bb640c-8719-4095-9bf5-ceeb7051a5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The colors of the rainbow are ROYGBIV.',\n",
       " \"Elsie works the register at Arby's.\",\n",
       " 'China has 1.4 billion people.',\n",
       " 'The time is 3:14.',\n",
       " 'The largest continent is Asia.',\n",
       " 'Christiano Ronaldo']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "975f0dac-6bfd-4e3b-b6e3-e432122a2dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 2}, page_content='China has 1.4 billion people.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Which country has the largest population?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdfd53f-3620-4fa6-bc65-6d2466f76ca6",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "Now letâ€™s apply the same retrieval process to a file we read in. The file `workplaces.txt` contains names and workplaces of several people. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca03765b-8e45-4db0-adcc-3b9eb2803f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Aaron works at McDonald's\", 'Beth works at Starbucks', 'Charlie works at Walmart', 'Daisy works at Amazon']\n"
     ]
    }
   ],
   "source": [
    "with open(\"workplaces.txt\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "lines = [line.strip() for line in lines]\n",
    "print(lines[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062cf178-d4c9-4910-845f-13f700a120e2",
   "metadata": {},
   "source": [
    "`workplace_retriever` is a function that takes in the workplace.txt file and returns a database as retriever that you can use to find out the workplaces of people in the file. You can specify the top-k results in the argument of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dead1bb8-2f88-417d-9a22-4de7f9685354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def workplace_retriever(k=3):\n",
    "    with open(\"workplaces.txt\", 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "    db = Chroma.from_texts(lines,vectorizer, metadatas=[{\"id\": id} for id in list(range(len(lines)))])\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1679c17b-1943-4558-aec0-0a201c84a906",
   "metadata": {},
   "source": [
    "Using `workplace_retriever`, **find out who works at Starbucks and McDonald's**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfd4b386-f9fb-437f-8951-ad8400adcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find out who works at Starbucks and who works at McDonalds. Use the retriever(k=3).invoke(<query>) method to do this\n",
    "# You can experiment with the value of k to make sure you find all people that work in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "faa40354-ce6a-4313-a108-806402de4f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees at Starbucks:\n",
      "page_content='Brian works at Starbucks' metadata={'id': 27}\n"
     ]
    }
   ],
   "source": [
    "# Query for employees at Starbucks\n",
    "results_starbucks = retriever.invoke(\"Who works at Starbucks?\")\n",
    "print(\"Employees at Starbucks:\")\n",
    "for result in results_starbucks:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c42cf44b-85f5-4576-9472-cedc965aa288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees at McDonald's:\n",
      "page_content='Alice works at McDonald's' metadata={'id': 26}\n"
     ]
    }
   ],
   "source": [
    "# Query for employees at McDonald's\n",
    "results_mcdonalds = retriever.invoke(\"Who works at McDonald's?\")\n",
    "print(\"Employees at McDonald's:\")\n",
    "for result in results_mcdonalds:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747bcccb-8dd3-4fc2-ba37-2c10ae4b409d",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "#### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6852173-e73d-4571-a1b8-6d3342a51117",
   "metadata": {},
   "source": [
    "The `workplaces.txt` data we just looked at was conveniently split into lines, with each line representing a distinct and meaningful chunk of information. This straightforward structure makes it easier to process and analyze the text data.\n",
    "\n",
    "However, it is usually not so straightforward:\n",
    "- When dealing with text data, especially from large or complex documents, it's essential to handle the formatting and structure efficiently.\n",
    "- If we get a not-so-simply formatted file, we can break it down into manageable chunks using LangChain's `TextLoader` and `RecursiveCharacterTextSplitter`.\n",
    "- This allows us to preprocess and chunk the data effectively for further use in our RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa4dd5-660b-4586-b4c0-934944f3233e",
   "metadata": {},
   "source": [
    "Lets take a look at some of the *TIDE* documentation [here](https://tide.sdsu.edu/l). We have downloaded the contents of this webpage into two text files named `tide_doc_1.txt` and `tide_doc_2.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90806506-2a56-4a27-bd74-ae0d44f29abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Launch Server: Use your campus credentials to access the TIDE JupyterHub and launch your computational environment.', 'Jobs', 'For tasks that require extended computation time or more complex configurations, TIDE allows the execution of containers as jobs within namespaces. Hereâ€™s how you can manage these jobs:', '', 'Purpose:', '', 'Long-Running Tasks: Execute long-running or resource-intensive jobs using containers.', 'Namespaces:', '', 'Organization: Namespaces help in organizing users, jobs, and other resources within the TIDE environment.', 'Assistance:', '', 'Support: If you need help with creating or managing namespaces, submit a TIDE Support Request, and the team will assist you in setting up the necessary configurations.', 'Quick Links', 'TIDE Support Request: Submit a support request for any issues or queries.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"tide_doc_1.txt\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "lines = [line.strip() for line in lines]\n",
    "print(lines[20:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9062ca2-37f0-4aee-95fd-9e70dbe8bd45",
   "metadata": {},
   "source": [
    "- We see that the data and text is not split into meaningful chunks of information by default, so we need to try out best to format it in such a way it can be useful. This is why we use chunks, which capture local and neighboring texts, grouping them together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1e0e3a-3093-4fcf-ae68-a2f92c3c3642",
   "metadata": {},
   "source": [
    "A function that chunks `tide.txt` has been provided below.  When using the RecursiveCharacterTextSplitter, the chunk size determines the maximum size of each text chunk. This is particularly useful when dealing with large documents that need to be split into smaller, manageable pieces for better retrieval and analysis.\n",
    "\n",
    "**Experiment with different chunk sizes** and pick a size that captures enough information to answer the question: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "516bc647-7988-4aa5-9154-2e72af5367a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tide_retriever(chunk_size):\n",
    "    loader = TextLoader('tide_doc_1.txt')\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=10, separators=[\" \", \",\", \"\\n\"])\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    db = Chroma(embedding_function=vectorizer)\n",
    "    db.add_documents(texts)\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "251c533f-4fa7-4f60-b7bb-8ea894404e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Think about how many characters would be needed to contain useful information for such a complex task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02367974-88ac-48c0-8fdc-b8c1499b20cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'expanse_doc_1.txt'}, page_content='the TIDE environment.\\nAssistance:\\n\\nSupport: If you need help with creating or managing namespaces, submit a TIDE Support Request, and the team will assist you in setting up the necessary configurations.\\nQuick Links\\nTIDE Support Request: Submit a support request for any issues or queries.\\nTIDE JupyterHub: Access the JupyterHub interface for managing computational resources.\\nTIDE on GitHub: Explore the TIDE documentation and additional resources on GitHub.\\nTIDE YouTube: Watch tutorials and informational videos about TIDE.\\nNSF Award #2346'),\n",
       " Document(metadata={'source': 'expanse_doc_1.txt'}, page_content='TIDE Wave\\nTechnology Infrastructure for Data Exploration (TIDE)\\nTIDE is an advanced infrastructure platform integrated into the National Research Platform Nautilus hyper-cluster. It provides a robust environment for data exploration and computational tasks, leveraging cutting-edge technology to support artificial intelligence, machine learning, and data science projects.\\n\\nUsing TIDE\\nJupyterHub\\nJupyterHub is a web-based interface provided by TIDE that allows users to access a range of computational resources and software tools. Hereâ€™s how to get started:\\n\\nAccess:\\n\\nWeb Interface: JupyterHub is accessible from any web browser, providing a user-friendly interface for managing and running computational notebooks.\\nFeatures:\\n\\nPre-Configured Containers: Users can choose from a variety of pre-built software containers that include packages and tools commonly used in AI, ML, and data science.\\nGraphical Processing Units (GPUs): Resources include options for GPUs, which are essential for'),\n",
       " Document(metadata={'source': 'expanse_doc_1.txt'}, page_content='for high-performance computing tasks.\\nGetting Started:\\n\\nRequest Access: Complete the TIDE Support Request form to obtain access to the JupyterHub instance.\\nQuickstart Guide: Review the Quickstart guide available on the TIDE GitHub documentation site to familiarize yourself with the platformâ€™s features and usage.\\nSoftware Containers: Explore the list of available software container images. For any queries or additional needs, submit a TIDE Support Request.\\nLaunch Server: Use your campus credentials to access the TIDE JupyterHub and launch your computational environment.\\nJobs\\nFor tasks that require extended computation time or more complex configurations, TIDE allows the execution of containers as jobs within namespaces. Hereâ€™s how you can manage these jobs:\\n\\nPurpose:\\n\\nLong-Running Tasks: Execute long-running or resource-intensive jobs using containers.\\nNamespaces:\\n\\nOrganization: Namespaces help in organizing users, jobs, and other resources within the TIDE')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "tide_retriever(1000).invoke(\"WHat is TIDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a48cb82-d6a2-4785-a68d-12437236633c",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "#### Multiple Document Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68743bad-9b2d-44c1-9cb0-1f4203869467",
   "metadata": {},
   "source": [
    "When we have more than one document we want to use in our database, we can simply iteratively chunk them. Metadata for the text source is added by default, but we can add our own metadata as well in the form of IDs.\n",
    "\n",
    "\n",
    "`tide_all_retriever` is a function that chunks both `tide_doc_1.txt` and `tide_doc_2.txt` has been provided below, using a chunk size of 1000 characters, **Find which document information for \"Compiling Codes\" is most likely to be located.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d746a3dd-e674-41c3-b0f8-22856530f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tide_all_retriever(chunk_size):\n",
    "    import glob\n",
    "    db = Chroma(embedding_function=vectorizer)\n",
    "    pattern = 'tide_doc_*.txt'\n",
    "    file_list = glob.glob(pattern)\n",
    "    for file_name in file_list:\n",
    "        loader = TextLoader(file_name)\n",
    "        documents = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=10, separators=[\" \", \",\", \"\\n\"])\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        for id,text in enumerate(texts):\n",
    "            text.metadata[\"chunk_number\"] = id\n",
    "        db.add_documents(texts)\n",
    "\n",
    "    \n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "642fd129-85b9-4909-92e4-212cee3345ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the relevant source for the query \"Compiling Codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdf79abe-d9ca-4569-bea4-f21440df1e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk_number': 0, 'source': 'expanse_doc_2.txt'}\n",
      "{'chunk_number': 1, 'source': 'expanse_doc_2.txt'}\n",
      "{'chunk_number': 5, 'source': 'expanse_doc_2.txt'}\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "chunks = tide_all_retriever(1000).invoke(\"Compiling Codes\")\n",
    "for chunk in chunks:\n",
    "    print(chunk.metadata)\n",
    "\n",
    "# The answer is tide_doc_2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092f17f-faeb-4fee-8a00-3697a71988fa",
   "metadata": {},
   "source": [
    "## Part 2: Basic RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66182b0-291a-4913-803c-1f2894af1310",
   "metadata": {},
   "source": [
    "Ollama is an open-source LLM platform that allows us to use a plethora of different LLMs. In this notebook, Mistral is our LLM of choice. Feel free to play around with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "114e1036-dfc4-4ebf-abb8-b2da98b03e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm just a computer program, so I don't have feelings or emotions. I'm here to help you with any questions or problems you might have! How can I assist you today?\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama = Ollama(model=\"mistral\")\n",
    "ollama.invoke(\"How are you doing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fae322-2770-45c2-a3d9-245f6ecaae55",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "**Write a function that uses the `workplace_retriever` function to parse your question, retrieves relevant responses from `workplace_retriever`, and then sends this context to Ollama for it to answer your question in natural language.** Fill in `workplace_question` which accomplishes this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88b53c73-8369-4e89-8190-9862ebc93483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def workplace_question(question):\n",
    "    retriever = workplace_retriever()\n",
    "    context = retriever.invoke(question)\n",
    "    ollama = Ollama(model=\"mistral\")\n",
    "    prompt = f\"Based on the following context: {context}, answer the question: \"\n",
    "    response = ollama.invoke(prompt + question)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c70a7fba-ef08-41cf-861b-255f76fcea28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Brian works at Starbucks.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workplace_question(\"Who works at Starbucks?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f0ca36-fcf4-449d-a21e-3e9310e39eaa",
   "metadata": {},
   "source": [
    "## Part 3: LangChain RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fda8a1-2152-4d38-bbee-775ba0d49110",
   "metadata": {},
   "source": [
    "The above is a very simple example of a RAG. Now, using langchain, we can put everything together in a cleaner and all inclusive way in one go. Let's combine everything we've learned into the function `generate_rag`.\n",
    "\n",
    "- The below implementation has a custom class that allows us to view what chunks are being used based on our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bedb5d18-0871-490b-b7d9-42c582918150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag(verbose=False, chunk_info=False):\n",
    "    import glob\n",
    "    vectorizer = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    db = Chroma(embedding_function=vectorizer)\n",
    "    pattern = 'tide_doc_*.txt'\n",
    "    file_list = glob.glob(pattern)\n",
    "    for file_name in file_list:\n",
    "        loader = TextLoader(file_name)\n",
    "        documents = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10, separators=[\" \", \",\", \"\\n\"])\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        for id,text in enumerate(texts):\n",
    "            text.metadata[\"chunk_number\"] = id\n",
    "        db.add_documents(texts)\n",
    "    \n",
    "    template = \"\"\"<s>[INST] Given the context - {context} </s>[INST] [INST] Answer the following question - {question}[/INST]\"\"\"\n",
    "    pt = PromptTemplate(\n",
    "                template=template, input_variables=[\"context\", \"question\"]\n",
    "            )\n",
    "    # Let's retrieve the top 3 chunks for our results\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "    class CustomRetrievalQA(RetrievalQA):\n",
    "        def invoke(self, *args, **kwargs):\n",
    "            result = super().invoke(*args, **kwargs)\n",
    "            if chunk_info:\n",
    "                # Print out the chunks that were retrieved\n",
    "                print(\"Chunks being looked at:\")\n",
    "                chunks = retriever.invoke(*args, **kwargs)\n",
    "                for chunk in chunks:\n",
    "                    print(f\"Source: {chunk.metadata['source']}, Chunk number: {chunk.metadata['chunk_number']}\")\n",
    "                    print(f\"Text snippet: {chunk.page_content[:200]}...\\n\")  # Print the first 200 characters\n",
    "            return result\n",
    "    rag = CustomRetrievalQA.from_chain_type(\n",
    "        llm=Ollama(model=\"mistral\"),\n",
    "        retriever=retriever,\n",
    "        memory=ConversationSummaryMemory(llm=Ollama(model=\"mistral\")),\n",
    "        chain_type_kwargs={\"prompt\": pt, \"verbose\": verbose},\n",
    "    )\n",
    "\n",
    "    return rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f85b1e-4d5e-46b7-9fc4-82a993b2af2d",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "**Compare how mistral performs without context, and with context, i.e. without RAG and with RAG.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27716e1a-d3e7-4a3d-aa75-27a29919d06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To check the available resources on an Expanse Supercomputer, you would typically use command-line tools provided by the system. Here's a general guide:\n",
      "\n",
      "1. **SSH into the supercomputer**: First, you need to establish a secure connection to the supercomputer using SSH (Secure Shell). You'll need the hostname or IP address of the supercomputer and your credentials. The command would look something like this:\n",
      "\n",
      "   ```\n",
      "   ssh username@supercomputer_hostname\n",
      "   ```\n",
      "\n",
      "2. **Check CPU usage**: To check the CPU usage, you can use the `top` command. This command will display a dynamically updated list of processes on your system sorted by their resource utilization.\n",
      "\n",
      "   ```\n",
      "   top\n",
      "   ```\n",
      "\n",
      "3. **Check memory usage**: To check the memory (RAM) usage, you can use the `free -h` command. This command will display information about the total amount of free and used memory in a human-readable format.\n",
      "\n",
      "   ```\n",
      "   free -h\n",
      "   ```\n",
      "\n",
      "4. **Check disk usage**: To check the disk usage, you can use the `df -h` command. This command will display the amount of disk space used and available on each filesystem.\n",
      "\n",
      "   ```\n",
      "   df -h\n",
      "   ```\n",
      "\n",
      "5. **Check job status**: If you're running jobs on the supercomputer (e.g., using SLURM or Torque), you can check their status with appropriate commands. For example, with SLURM:\n",
      "\n",
      "   ```\n",
      "   squeue -u your_username\n",
      "   ```\n",
      "\n",
      "Remember, the exact commands might vary depending on the specific supercomputing system you're using and its configuration. Always consult the documentation or administrators for more precise information.\n"
     ]
    }
   ],
   "source": [
    "print(ollama.invoke(\"What is TIDE, and what are its primary focuses?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1dbe7824-f704-4dac-ba1a-55b872a66e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To check available resources on the Expanse Supercomputer, you can use the following command in your terminal after loading the required module (sdsc) and navigating to the correct directory:\n",
      "\n",
      "```bash\n",
      "[user@login01 ~]$ module load sdsc\n",
      "[user@login01 ~]$ expanse-client resource\n",
      "```\n",
      "\n",
      "This will display a table showing available resources, including the resource name, project details, usage, and availability. If you want to check the resources for a specific project or resource, you can use the 'user' parameter followed by '-r' to specify the desired resource:\n",
      "\n",
      "```bash\n",
      "[user@login01 ~]$ expanse-client user -r <resource>\n",
      "```\n",
      "\n",
      "Replace '<resource>' with the name of your desired resource or leave it blank to view data for the default resource.\n"
     ]
    }
   ],
   "source": [
    "tide_rag = generate_rag()\n",
    "result = tide_rag.invoke(\"How do you check available resources on tide Supercomputer\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44d0c4-a5fa-46c5-84a2-16ecc83a42c2",
   "metadata": {},
   "source": [
    "**We can see what is exactly being passed into the LLM highlighted in green when we set `verbose` to True.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b21255-05d4-47d1-a1a7-5624f24e7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc228f4-d6b8-427d-b49b-78214fa606ec",
   "metadata": {},
   "source": [
    "#### Great work! We've officially made a chatbot that can help us out with all things *TIDE*, at least according to the 2 .txt files we have access to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f1d699-f672-4f64-8261-78cbcc58dd16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llm]",
   "language": "python",
   "name": "conda-env-llm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
